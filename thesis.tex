%%% This is the master's thesis for William Woodall
%%%  It is based on the Auburn Thesis template by Chris Wilson

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Document Configuration
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[12pt]{report}
\usepackage{aums}        % For Master's papers
% \usepackage{auphd}       % For Ph.D.
% \usepackage{auhonors}    % For honors college
\usepackage{ulem}        % underlining on style-page; see \normalem below
\usepackage{url}
% \usepackage{tikz}
% \usepackage{pgf}
\usepackage{amsmath}
\usepackage{cite}
\usepackage{subfig}
% \usepackage{lineno}  % Inserts line numbers

%% For Algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

%% For graphics
\usepackage[pdftex]{graphicx}
\graphicspath{{figures/}}
\DeclareGraphicsExtensions{.pdf,.jpeg,.png}

% adjust size of line numbers
% \def\linenumberfont{\normalfont\small\sffamily}

%%%%% Format rules: Normal margins are 1 in.
%%%%% If you need to print with 1.5in margins, uncomment the line below
% \oddsidemargin0.5in \textwidth6in

%% If you do not need a List of Abbreviations,
%% then comment out the lines below and the \printnomenclature line.
%% for List of Abbreviations information:
%%  (see http://www.Mackinac.com/TECHTALK/509.htm  )
\usepackage[intoc]{nomencl}
\renewcommand{\nomname}{List of Abbreviations}           
\makenomenclature 
%% don't forget to run:   makeindex ausample.nlo -s nomencl.ist -o ausample.nls

% May want theorems numbered by chapter
\newtheorem{theorem}{Theorem}[chapter]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Title Page Configuration
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Put the title, author, and date in. 
\title{3D Teleoperation with the Microsoft Kinect}
\author{William J. Woodall IV}
\date{August 4th, 2012} % date of graduation
\copyrightyear{2012} % copyright year

\keywords{teleoperation, mapping, robotics, octree, Kinect}

% Put the Thesis Adviser here.
\adviser{Sa\^{a}d Biaz}

% Put the committee here (including the adviser), one \professor for each. 
% The advisor must be first, and the dean of the graduate school must be last.
\professor{Sa\^{a}d Biaz, Chair, Associate Professor of Computer Science and Software Engineering}

\professor{David M. Bevly, Co-Chair, Albert Smith Jr. Professor of Mechanical Engineering}

\professor{John Y. Hung, Professor of Electrical and Computer Engineering}

\begin{document}

% \linenumbers  % turn on line numbers for editorial needs

\begin{romanpages}      % roman-numbered pages 

\TitlePage 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Abstract
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract} 
This thesis investigates the feasibility of using novel three dimensional sensors like the Microsoft Kinect\cite{KINECT} and the Asus Xtion Pro Live\cite{ASUS} to assist the teleoperation of mobile vehicles.  Ultimately this work would be applicable to any teleoperated vehicle equipped with sensors providing three dimensional data of the environment, such as an automated ATV with a stereo vision system or a Velodyne LiDAR\cite{halterman2010velodyne} system.  The challenges related to utilizing dense three dimensional data in a way that is practical for teleoperation scenarios are identified, and solutions are proposed and implemented.  To simplify the approach, the problem is split into three smaller tasks: three dimensional mapping, teleoperation and telemetry visualization, and latency reduction techniques.  The three dimensional mapping pertains to using the three dimensional sensor data in concert with the mobile vehicle navigation solution to generate a three dimensional map of the environment in real-time.  The resulting map must be efficiently sent to the teleoperator and visualized in the teleoperation and telemetry visualization section of the thesis.  Additionally, latency greatly reduces the teleoperator's ability to drive the vehicle, so methods for reducing the perceived latency are investigated, including using a vehicle model to simulate the vehicle motion in the absence of timely telemetry updates.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Acknowledgments
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{acknowledgments}
Acknowledge people here.
% TODO: this
\end{acknowledgments}

\tableofcontents
\listoffigures
\listoftables
\listofalgorithms

\printnomenclature[0.5in] %used for the List of Abbreviations
\end{romanpages}        % All done with roman-numbered pages


\normalem       % Make italics the default for \em

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Chapter: Introduction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Introduction}\label{chap:introduction}
This work is motivated by the need for improved teleoperation techniques in commercial and military applications.  Given three dimensional information about the environment, a much better representation of the environment can be presented to the teleoperator.  The reconstructed environment provides an advantage to traditional two dimensional images by providing spatial awareness and multiple view points of the same scene.  Applications of this sort of technology include: maneuvering a teleoperated vehicle in tight quarters like a city alley, teleoperating a robotic arm in a cluttered environment, or navigating an indoor robot in an office environment.  The advantages of this three dimensional map bring related disadvantages that must be minimized in many teleoperation scenarios to make the benefit worth the cost.  In most teleoperation scenarios things like computing resources, battery life, power, and bandwidth are at a premium.  These scarce resources are problematic when considering the three dimensional mapping system as it can use considerable computer resources to process the three dimensional data and produce a map of the environment.  Additionally, the resulting map can be quite large, depending on the resolution of the map and the size of the area being represented.  Thus, when considering the use of these three dimensional maps in teleoperation scenarios, techniques for minimizing the processing and bandwidth must be considered.  Furthermore, in many teleoperation scenarios the communications link is quite latent, due to being a satellite or cellular system.  The increased latency in a teleoperation system significantly reduces the teleoperator's ability to control the vehicle - impacting both their obstacle avoidance capabilities and top speeds\cite{photo_real}.

\section{Mapping}
Mapping three dimensional environments has become a hot research topic in the past few years, and some amount of that trend can be attributed to the Microsoft Kinect\cite{KINECT}.  Since its release in November of 2010\cite{GIZMODO}, the Kinect has enabled researchers and enthusiasts all over the world by giving them access to high quality three dimensional data in an available and affordable sensor package.  The experimental part of this work takes advantage of the innovation of Kinect technology and applies it to the teleoperation of mobile robot vehicles.

With the increasing availability of three dimensional spatial data from sensors like the Primesense based Red Green Blue-Depth (RGB-D) cameras \cite{PRIMESENSE} and registering sweeping laser range finders with cameras \cite{photo_real}, being able to store, transmit and manipulate this data in an efficient manner has become important.  There are a number of applications for this type of three dimensional information - for example: three dimensional or six dimensional Simultaneous Localization and Mapping (SLAM) \cite{biswasdepth}, three dimensional teleoperation \cite{photo_real}, and robotic path planning \cite{3DCOLLISION}.  Because of this surge in the availability of three dimensional data, commonly represented as a point cloud, a new open library aimed at manipulating and processing point clouds has risen to popularity.  This library is known as the the Point Cloud Library (PCL)\cite{rusu20113d}, and it provides many high quality data structures and algorithms for processing point clouds.
  
Outdoor and indoor environments can be difficult to capture in full three dimensional space. One of the issues with mapping three dimensional space is that the computing resources (like processing time, memory usage, and network bandwidth) can be prohibitive to storing, transmitting, and processing the map. Octrees have a long history of being used in three dimensional applications\cite{boada2001multiresolution}, surface reconstruction\cite{kazhdan2006poisson}, and computer graphics\cite{fang1996deformable}. Recent work has shown that three dimensional maps can be stored efficiently using octrees for use in robotics applications\cite{octomap}. The research by A. Hornung, et. al. resulted in an open source library called octomap \cite{octomap}. The primary use of the octomap library so far has been in occupancy based motion planning for robots and robotic manipulators \cite{3DCOLLISION}. This work applies the octree-based spatial database methods described in the octomap paper to mapping three dimensional environments with teleoperation of robotic vehicles in mind.

%% TODO: Add a section about teleoperation here to balance the mapping section.

The rest of this thesis will cover the system design process and resulting artifacts in Chapter \ref{chap:system_design}, the three dimensional mapping subsystem in Chapter \ref{chap:3d_mapping}, the teleoperation subsystem in Chapter \ref{chap:teleoperation}, discuss the techniques for latency reduction in Chapter \ref{chap:latency_reduction}, describe the experimental setup and results in Chapter \ref{chap:experiments}, and finally draw conclusions and look toward future work in Chapter \ref{chap:conclusion}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Chapter: System Design
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{System Design}\label{chap:system_design}
There are several groups that have previously aimed to provide a more photo realistic three dimensional environment in order to improve teleoperation scenarios.  In particular Huber, et. al. from Carnegie Mellon University describe a system with a multi faceted approach to representing the environment in three dimensional space\cite{photo_real}.  Their system showed that it was possible to achieve a high level of fidelity when representing the environment.  They employed several techniques to reconstruct the environment including billboards, voxel grids, point clouds, and ground plane estimation and modeling.  Additionally, they showed that their system improved teleoperation of automated vehicles, and they characterized the affects of latency on the teleoperators.  Their work did not address the problems of processing or bandwidth.  Instead, their system had an umbilical fiber optic cable to transmit the telemetry back to an on-site trailer where the telemetry was converted into the three dimensional components using a rack of several servers.  Therefore, one of the goals of this thesis is to address some of those concerns by simplifying the representation of the three dimensional environment and by leveraging different storage structures and methodologies when building the map and transmitting it to the teleoperator.

\section{Problem Statement and Requirements Analysis}
The system should take telemetry from the robotic vehicle and use it to produce a visually accurate three dimensional map of the environment.  The resulting map should be displayed to the user, also known as the teleoperator, and the input of the user should be used to control the robotic vehicle.  The telemetry should include both proprioceptive data which will provide information about the location of the vehicle in its environment and exteroceptive data which provides three dimensional data about the environment.  By combining these two types of data, a three dimensional model of the vehicle is placed in a reconstruction of the environment and is displayed to the teleoperator.  The mechanism by which the reconstructed environment is transmitted to the teleoperator must work in low bandwidth and high latency networks.

\subsection{System of Systems Architecture}
The system design follows from the problem statement and the requirements analysis as a system of systems in a client-server architecture.  The system can clearly be divided into a robotic vehicle subsystem, a three dimensional mapping subsystem, and a teleoperation subsystem.  The robotic vehicle subsystem can be abstracted and therefore decoupled from the mapping and teleoperation subsystems.  The three dimensional mapping system takes the abstracted telemetry from the robotic vehicle with which it maintains a three dimensional map of the environment.  This map is then provided to the teleoperation system which transmits the map and visualizes it for the teleoperator.  Additionally, the teleoperation subsystem takes the user input which is communicated to the command interface of the robotic vehicle.  The teleoperator and his machine can be viewed as the client, and the robotic vehicle is the server which provides processed telemetry and executes input from the teleoperator.  A high level diagram of the system is seen in Figure \ref{fig:subsystem}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=5in,keepaspectratio]{subsystem.pdf}
  \caption{Subsystem Layout}
  \label{fig:subsystem}
\end{figure}

An assumption about the design is made in the above paragraph.  The assumption is that all of the telemetry is processed by the mapping subsystem before being transmitted by the teleoperation subsystem.  It would be possible to transmit the raw telemetry to the teleoperator and then process it on the teleoperator's machine, but this was not the selected design pattern because the network bandwidth and latency was more valued than the processing power on the robotic vehicle.  This design attempts to meet the lowest common denominator by minimizing processing for the mapping and then transmitting the completed map, which can be much more concise than the raw telemetry.

By decoupling the three subsystems, replacing one implementation of a subsystem with another is much easier.  Because the interface of the robotic vehicle consist of one or more exteroceptive sensors, a proprioceptive pose solution, and a velocity command interface, the underlying robotic vehicle can be replaced with any robotic subsystem that can conform to that interface.  Similarly, the mapping subsystem can be replaced with any individual mapping system that takes the telemetry from the robotic vehicle and produces a map for the teleoperation subsystem.

\section{System Specification}
Breaking the subsystems down further, implementation specific details begin to emerge.  Figure \ref{fig:system_diagram} shows the three subsystems in more detailed and more tightly integrated figure.  Though the interfaces are missing here the flow of information remains the same from the subsystem layout in Figure \ref{fig:subsystem}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=6in,keepaspectratio]{system_diagram.pdf}
  \caption{System Design}
  \label{fig:system_diagram}
\end{figure}

As mentioned above, Figure \ref{fig:system_diagram} describes the overall system design.  The three dimensional data in the form of point clouds are collected from one or more sensors like the Microsoft Kinect.  These point clouds are combined with a navigation solution from the robotic base to transform the data points into a common map coordinate frame.  At this point the point clouds are inserted into the map using a probabilistic insertion method which is described in Chapter \ref{chap:3d_mapping}.  A maximum likelihood version of the map is generated and is stored in, what this work will refer to as, the server map.  Now the map is ready to be used in teleoperation, so the server map is subtracted from the current client's map.  The differences, or part of the differences, are sent to the client and united with the current client map.  If all of the differences are sent, then the client map and the server map should then be synchronized, i.e. the client map is up-to-date.  Otherwise successive iterations of this process will eventually result in an up-to-date client map.  Simultaneously, input from the teleoperator is sent to the robotic vehicle to allow teleoperation.

The above description, or theory of operation, of the system makes references to several as yet undefined processes and design decisions.  There are more details on the Mapping and Teleoperation subsystems in Chapters \ref{chap:3d_mapping} and \ref{chap:teleoperation}, respectively.  The robotic subsystem and it aforementioned navigation solution are discussed in the next section and the Experimental Setup in Section \ref{sec:robotic_vehicle}.

% TODO: describe the Sick LMS151
\section{Robot Navigation}
A necessary component of the proposed teleoperation system is the ability for the robotic vehicle to provide an accurate pose estimate.  Because the system proposed does not rely on Iterative Closest Point (ICP) or feature mapping to combine successive point clouds, as in P. Henry, M. Krainin, and E. Herbst\cite{Henry2010}, the pose estimate from the robotic vehicle must be very accurate.  The trade off is that a much smaller amount of processing is required to generate the map, but the resulting quality of the map is sensitive to errors in the pose estimate.  Outdoor navigation systems like the Novatel SPAN system can provide $\sim$2 centimeters positional accuracy and 1/10th degree angular accuracy\cite{kennedy2006architecture}, which would be acceptable for mapping.  Indoor robots, like the experimental setup for this paper, require some other means of estimating its position in an arbitrary global coordinate frame.  The experimental system in this work uses an implementation of grid SLAM, which combines odometry from the wheel encoders with laser range finder data from a Sick LMS151.  An off-the-shelf SLAM library is used to get a high quality pose estimate indoors, and the library uses a grid based approach with particle filters to map and localize the environment.  The library is called the gmapping library\cite{grisetti2007improved}\cite{grisettiyz2005improving}, which is an open source library and can be found on openslam.org.  These navigation systems give a high accuracy, drift free pose solution of the vehicle indoors.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Chapter: Three Dimensional Mapping
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Three Dimensional Mapping}\label{chap:3d_mapping}
As with the top level system design, other mapping solutions are analyzed before design begins.  In the following section a survey of past work is presented.

\section{Previous Work}
\label{sec:previouswork_3dmapping}
Recent work has been done to create three dimensional maps of the environment using octrees and a probabilistic insertion method that is well suited for robotic activities where noise is present in the data and uncertainty in the pose of the robot. A. Hornung, et. al. described and implemented this system, and it resulted in an open-source library called octomap.\cite{octomap} This work uses octomap as the octree mapping system, and exactly how that works is described in section \ref{sec:3dmapping}.  There are two main elements of the octomap research that are useful in a three dimensional teleoperation system.

First, the probabilistic method for adding new three dimensional information to the map is ideal for a system that needs to be updated continuously.  Uncertainty from the navigation solution will result in point clouds that are transformed incorrectly, and this causes inconsistencies and skews in the resulting map. The probabilistic manner in which the scans are inserted into the octree help to alleviate this inconsistency by allowing for some error in the point clouds in relation to the map. This error is further alleviated by the nature of the octree data structure because inserting the point clouds into the octree essentially results in a down-sampling of the original data to the resolution of the octree.

Second, limiting the query depth on the octree will effectively and efficiently down-sample the map.  The down-sampling allows for an adjustable quality level of the map being sent over the wire to match the available resources like network bandwidth and network latency.  Having adjustable quality is important because most teleoperation systems work over a wireless and unreliable communication layer, which often suffers from low data rates and large latencies.  An analogy can be drawn to how on-line video streaming services will adjust the compression of a video to accommodate the connection being used, but in these systems the main concern is bandwidth, not latency.

In addition to the work done by A. Hornung, et. al. with octomap, three dimensional mapping done specifically with RGB-D cameras from Primesense was done by P. Henry, M. Krainin, and E. Herbst\cite{Henry2010}.  In this paper they used alignment techniques like features with RANdom SAmple Consensus (RANSAC), Iterative Closest Point (ICP), and loop closure techniques to produce a very high quality map from the RGB-D data\cite{Henry2010}.  The mapping approach in this thesis differs in that it relies on other sensors and existing navigation solutions to provide accurate transforms for the points clouds in an attempt to make the system more processor efficient and run closer to real-time.  The work by P. Henry, M. Krainin, and E. Herbst still managed to perform the processing in a relatively small amount of time, but processing in that paper has not gotten it up to the speeds required for real-time teleoperation.

Very recent work by Microsoft using the Kinect has achieved real-time mapping with the Kinect in a project called KinectFusion\cite{izadi2011kinectfusion}.  Additional work has seen this technology demo reproduced using PCL and extended spatially\cite{whelankintinuous}.  This technique employs new methods for combining Kinect data on the Graphics Processing Unit of the video card in the computer.  This technology is very immature, but most likely will be a suitable replacement for the mapping system presented in this work.  The advantage to the mapping system presented in this work is that it requires less processing than the KinectFusion system.

\section{The Mapping Process}
\label{sec:3dmapping}
This section describes the process of combining the three dimensional data from the sensor and six dimensional poses from the robot base into three dimensional maps of the environment. As previously mentioned in Section \ref{sec:previouswork_3dmapping}, A. Hornung, et. al. have already shown that octrees combined with a probabilistic insertion method can provide a good solution when creating three dimensional maps. A portion of this section explains their work and how it is used in the teleoperation system described in this thesis work.\cite{octomap} The obvious first step of this process is obtaining the three dimensional data and the six dimensional pose of the robotic base from which the map is constructed, but these are described briefly in Section \ref{sec:robotic_vehicle}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=5in,keepaspectratio]{transforms.pdf}
  \caption{Typical Coordinate Transform Tree}
  \label{fig:transforms}
\end{figure}

\subsection{Transforming the Point Clouds}
Each time new data from the three dimensional sensor is received by the computer the data needs to be transformed into a common arbitrary global frame or `map' frame. Figure \ref{fig:transforms} shows a typical transformation tree for the robotic setup. The `base\textunderscore{}link' frame is commonly referred to as the vehicle frame. The transformation between the `map' frame and the `base\textunderscore{}link' frame represents absolute transformation given by the pose estimate of the robotic vehicle. The transformations between the `base\textunderscore{}link' frame and the `laser\textunderscore{}link' frame and the `camera\textunderscore{}link' frame are static geometric relationships and represent the position of the sensors on the robot chassis. When new point clouds are received they are in the `camera\textunderscore{}link' coordinate frame and need to be converted into the `map' coordinate frame as that is the corrected global frame in this situation.

In order to transform the point clouds into the global coordinate frame, each point in the point cloud must be transformed into the global coordinate frame.  Each point is represented by the homogeneous vector in Equation \ref{eq:point}.

\begin{equation} \label{eq:point}
\left[ \begin{array}{c} x \\ y \\ z \\ 1 \end{array} \right]
\end{equation}

The points are transformed using the homogeneous transform matrix\cite{lavalle2006planning}, defined in Equation \ref{eq:transform_matrix}.

\begin{equation} \label{eq:transform_matrix}
\left[ \begin{array}{cccc} \cos \alpha \cdot \cos \beta  & \cos \alpha \cdot \sin \beta \cdot \sin \gamma \; -\; \sin \alpha \cdot \cos \gamma  & \cos \alpha \cdot \sin \beta \cdot \cos \gamma \; +\; \sin \alpha \cdot \sin \gamma  & x_{t} \\ \sin \alpha \cdot \cos \beta  & \sin \alpha \cdot \sin \beta \cdot \sin \gamma \; +\; \cos \alpha \cdot \cos \gamma  & \sin \alpha \cdot \sin \beta \cdot \cos \gamma \; -\; \cos \alpha \cdot \sin \gamma  & y_{t} \\ -\sin \beta  & \cos \beta \cdot \sin \gamma  & \cos \beta \cdot \cos \gamma & z_{t} \\ 0 & 0 & 0 & 1 \end{array} \right]
\end{equation}

In the homogeneous transform matrix $\gamma$ is Roll about the $x$-axis, $\beta$ is pitch about the $y$-axis, and $\alpha$ is Yaw about the $z$-axis.  $x_{t}$, $y_{t}$, and $z_{t}$ are the translational components of the transform matrix.

By multiplying each of the points in the point cloud by this transform matrix, the entire point cloud is transformed into the global coordinate frame and is ready to be inserted into the map.

\subsection{Inserting New Data}
% TODO: describe ray tracing
Once the point cloud data has been transformed into the global coordinate frame, the data needs to be added to the map. As previously mentioned, inserting data is done by the octomap library, but the process is described here for clarity. The underlying data structure for this map is an octree where each leaf has a probability of occupation. The octree is in effect a three dimensional occupancy grid with octree storage where each leaf of the tree is a voxel in the grid. The transformed data and the origin of the sensor in the global coordinate frame are required for the insertion. The insertion method starts by iteratively ray tracing from the origin of the sensor to each data point in the point cloud.  For each voxel that the ray trace passes through the probability of occupation of that voxel is decreased by a given amount. For the voxel that the ray trace ends in, the probability of that voxel being occupied is increased by a given amount. In this way several points must fall into a voxel to have voxel considered to be occupied, and allows for fringe voxels, caused by noise, to be cleared by additional new data.

\begin{figure}[ht]
  \centering
  \includegraphics[width=5in,keepaspectratio]{raytrace.pdf}
  \caption{Two Dimensional Example of Voxel Raytracing.}
  \label{fig:voxel_raytrace}
\end{figure}

Figure \ref{fig:voxel_raytrace} shows this process in a two dimensional example where the gray voxels are the robot, the red dot is the sensor origin, the black dot is the point from the point cloud, the green voxels are having their occupancy decreased, and the red voxel is having its occupancy increased. This method of voxel ray tracing is originally proposed by Amanatides, J. and Woo, A., and is referenced in the octomap paper\cite{amanatides1987fast}.

\section{Sources of Error During Three Dimensional Mapping}
With this approach to three dimensional mapping, map quality can be adversely affected by several different sources of error, including the uncertainty in the pose estimate of the robotic vehicle, random and systematic error in the three dimensional data, and systematic errors due to timing and geometric misalignment.  Figure \ref{fig:slamvsoctree} shows the resulting octree map with the map from the Simultaneous Localization and Mapping (SLAM) algorithm, which is good enough in this instance to be considered truth. The octree map generally lines up with the SLAM map but has a lot of areas where the result is less than desirable.

\begin{figure}[ht]
  \centering
  \includegraphics[width=4in,keepaspectratio]{slamvsoctree.pdf}
  \caption{A top down, orthographic view of a three dimensional map generated from Kinect data with a map created by the SLAM library}
  \label{fig:slamvsoctree}
\end{figure}

\subsection{Vehicle Pose Uncertainty}
\label{sec:uncertainty}
A major component of the error introduced in mapping with this system is the vehicle pose uncertainty.  In most robotic systems, and in the system presented in this thesis, the pose estimate is a blend of multiple sensors, some that drift and some that bound error growth.  In that case the pose estimate will have a cycle where the error grows until it is sharply bound by a measurement update.  In the system in this thesis, bounding occurs when a SLAM update occurs.  The SLAM algorithm provides updates to the pose of the vehicle at about 1 Hz, and between these updates the integrated wheel odometry is used to provide poses.  The integrated wheel odometry quickly diverges from the true path of the vehicle and the uncertainty grows quickly, especially when turning in place where the non-linear effects of slip are ignored in the integration of the encoders.  Therefore the uncertainty grows and then is sharply snapped back on a SLAM update.  This effect can be seen in Figure \ref{fig:uncertainty}, where the misalignment of the laser data, and the map becomes obvious.  After a SLAM update the pose estimate is much better.  This behavior presents a problem in the mapping; by inserting a point cloud just before the SLAM update, the point cloud transform into the map frame is going to be at its worst.

\begin{figure}[ht]
  \centering
  \includegraphics[width=6in,keepaspectratio]{uncertainty.png}
  \caption{Integrated wheel odometry diverging from the pose provided by SLAM when turning.}
  \label{fig:uncertainty}
\end{figure}

Another problem that comes up in the mapping process is that inserting point clouds into the octree map is computationally expensive.  The computation required is affected by the density of the point cloud data and the resolution of the map.  Some sensors, like the Kinect and Xtion Pro Live, produce this point cloud data at high rates, which can be as high as thirty hertz.  Due to the computational limitations and the high rate of speed of the data, many of the point clouds have to be dropped.  This is not necessarily a problem for mapping in the system presented here, because of the rate of the point cloud data and the speed of the robotic vehicle, 1-2 meters per second, much of the point cloud data is redundant.

A practical solution to both of these problems is to opportunistically select point clouds to insert.  In this system only point clouds that comes after SLAM updates are inserted.  Timing is a critical component at this stage, it is important to match SLAM transform timestamps to point cloud timestamps as closely as possible to avoid further misalignment in the map.

Another solution to this problem is to simply produce a better navigation solution.  Though not pursued in this work, combining the odometry with an Inertial Measurements Unit (IMU) or simply a yaw rate gyro with a Kalman Filter would likely reduce the error from the grow-bound cycle in the mapping process.  Point clouds would still need to be throttled, but which point clouds used would no longer matter as much.

\subsection{Sensor Random Error}
Another less prominent source of error is the random error in RGB-D cameras like present in the Kinect and the Xtion Pro Live.  Three sources of error in the this data are: loss of precision on depth measurements at greater distances, discretization error on depth measurements, and decreased density of data at greater distances.  The most prominent of these error sources is the variance in the depth measurement.  Khoshelham, K. showed that the random error in the depth of the Kinect data could be modeled from the theoretical principal of the depth sensor\cite{khoshelham2011accuracy}.  In the paper by Khoshelham, K., they show theoretically and experimentally that the depth variance at 4 meters is about 5 centimeters.  This variance changes over distance, with smaller distances yielding more precise measurements than at greater distances.

\begin{figure}[ht]
  \centering
  \includegraphics[width=6in,keepaspectratio]{variable_hit_miss.pdf}
  \caption{This figure demonstrates the variable hit and miss values used during ray tracing.}
  \label{fig:variable_hit_miss}
\end{figure}

In order to help minimize this error, a practical solution might be to ignore data that is further than 3 or 4 meters as the variance grows quite large.  The system in this work, however, makes a small modification to the method of insertion used by octomap.  Instead of having a constant hit and miss value for ray tracing each point in each point cloud, the hit and miss values change based on the distance to the origin of the sensor.  In this way data that is further from the sensor, especially greater than 3 or 4 meters, is less likely to make a voxel occupied and less likely to clear an occupied voxel.  The variable hit and miss values allow the system to incorporate data past 4 meters, but also takes into account the decreased precision.

Figure \ref{fig:variable_hit_miss} shows the effect of long and short ray casts with the above proposed changes.  In the figure the values are varied linearly by distance from the sensor origin, but in practice the system uses a quadratic scaling of the hit and miss values.  Additionally, the scaling does not start until 3 meters from the sensor.  These two changes from the simple linear scaling method reflect the fact that the data under 4 meters is quite good, and the data degrades non-linearly at greater distances.  The results of this tactic on mapping are noticeable in specific situations, but overall improvements are marginal compared to the mapping error induced by errors in the navigation solution.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Chapter: Teleoperation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Teleoperation}\label{chap:teleoperation}
While the three dimensional map is being continuously generated, the client needs to be updated regularly so that the teleoperator has up-to-date telemetry from which to make decisions about the commands to send to the robotic vehicle. This step is where the application of the three dimensional mapping to teleoperation occurs. Periodically the probability octree that represents the current best guess as to the occupancy of the environment is converted to a maximum likelihood tree.  This maximum likelihood tree is compared to the client's map of the environment.  The differences are sent over the wire to the client, where the differences are united with the current client map, effectively updating the map used for visualization.  Iterating these actions in parallel with the three dimensional mapping process allows for timely updates and a gracefully degrading difference model based on available bandwidth.

\section{Maximum Likelihood Representation}
The point clouds are initially inserted into an occupancy probability octree where the leaves contain the probability of occupation, but this representation is large and not suited for processing and transmitting the map over the wire in a low bandwidth and/or latent network. The map is therefore periodically transformed into a maximum likelihood version of the probabilistic octree. This transformation is performed by applying a probability high pass filter on the probabilistic octree, where the voxels that are most likely occupied are seen in the transformed octree. The maximum likelihood octree is much more compact because each leaf can be represented with exactly two bits each\cite{octomap}.  Two bits per leaf allows for four unique states for each leaf - of which three are utilized, occupied, unoccupied, or unknown.

\section{Map Streaming}
Once the map is in the maximum likelihood tree format, the map needs to be transmitted to the client. In some cases, however, there is more data to send than there is bandwidth available. In these cases a lower resolution version of the map differences can be sent and more detailed differences can be sent in the future when more bandwidth is available.

\subsection{Octree Set Difference}
The first step in synchronizing the server and client octrees is to detect what has changed since the last update. In order to determine this the client octree is set differenced with the server octree which yields the changes in the server from a previous point in time. In order to find the set difference between the two octrees Algorithm \ref{alg:octree_diff} is used. This algorithm is a simple element-wise difference and can be considered a volumetric difference.

\begin{algorithm}
\caption{Algorithm for Pairwise Difference of Octrees}
\label{alg:octree_diff}
\begin{algorithmic}
  \STATE \COMMENT {$O_0$ is the server occupancy tree}
  \STATE \COMMENT {$O_1$ is the client occupancy tree}
  \STATE \COMMENT {$d$ is the set difference}
  \FORALL{leafs in $O_0$}
    \IF {leaf not in $O_1$}
      \STATE $d\gets leaf$
    \ENDIF
  \ENDFOR
  \RETURN $d$
\end{algorithmic}
\end{algorithm}

\subsection{Bandwidth Adjustment Algorithm}
Once the differences have been determined, the differences need to be sent over the network to the client computer to be united with the current client map. If not enough bandwidth is available, the differences can be reduced by differencing the two octrees at a lower resolution. The lower resolution versions of the trees can be obtained by limiting the depth of the query into the octree. If the leaf size of the octree is 2 centimeters, then reducing the query depth by one will result in a subtree with leaves, 4 centimeters in size.  This phenomenon is demonstrated in a map of the Shelby Center at Auburn University by Figure \ref{fig:treedepth}.  The visualization in Figure \ref{fig:treedepth} was done using octovis\cite{octomap}, which is part of octomap.

\begin{figure}[ht]
  \centering
  \includegraphics[width=6in,keepaspectratio]{ShelbySE_2f_combined_octovis.png}
  \caption{Partial map of a hallway showing the effect of limiting the query 
           depth.  From left to right the resolution is 0.05m, 0.1m, 0.2m,
           and 0.4m.}
  \label{fig:treedepth}
\end{figure}

The algorithm for selecting the depth is proposed in Algorithm \ref{alg:bandwidth} and simply continues to degrade the resolution until enough bandwidth is available to transmit or until there resolution cannot be degraded further.

\begin{algorithm}
\caption{Algorithm for Determining Difference Depth}
\label{alg:bandwidth}
\begin{algorithmic}
  \STATE \COMMENT {$O_0$ is the server map octree}
  \STATE \COMMENT {$O_1$ is the client map octree}
  \STATE \COMMENT {$d$ is the set difference}
  \STATE \COMMENT {$B_d$ is the bandwidth required to transmit the diff}
  \STATE \COMMENT {$B_a$ is the available bandwidth}
  \STATE \COMMENT {$T_u$ is the update period in seconds}
  \STATE \COMMENT {$h_{max}$ is the maximum size of a leaf to be transmitted}
  \STATE $i\gets 0$
  \REPEAT
    \STATE $O_0 \prime = depth(O_0,height(O_0)-i)$
    \STATE $O_1 \prime = depth(O_1,height(O_1)-i)$
    \STATE $d = O_0 \prime \cup O_1 \prime $
    \STATE $B_d = size(d) / T_u$
    \STATE $B_a = currentAvailableBandwidth()$
    \STATE $i = i + 1$
  \UNTIL {$B_d\le B_a \| resolution(i) \ge h_{max}$}
  \RETURN $d$
\end{algorithmic}
\end{algorithm}

The depth function simply returns a subtree of the given tree at the given height, and the height function gives the height of the specified tree.  The resolution function simply returns the dimension of a leaf at the given level.  The current available bandwidth function could be implemented in several ways, but the topic of available bandwidth estimation has been covered in the literature\cite{prasad2003bandwidth}. The principle behind the bandwidth estimation includes metrics like round-trip-time, average delay, packet loss, and throughput to determine the bandwidth currently available.  During testing the bandwidth will be artificially controlled to exercise this component of the system.  Artificially controlling the bandwidth used by the mapping system will also allow for quality of service to be enforced.  Quality of service would allow the system to prioritize things like teleoperation commands, video, or other telemetry over the map update.

\subsection{Octree Set Union}
The final stage of synchronizing the server and client octrees is to unite the differences with the client octree. This algorithm is just a simple union operation and can be performed by simply inserting every voxel of the differences into the client map using the normal octree insertion method.\cite{meagher1982geometric}

\section{Visualization}
The purpose of mapping the environment and streaming that map to the client is so that the teleoperator might gain some improvement of control when teleoperating the robotic vehicle.  In addition to the map being sent to the client, the position of the robotic vehicle in the map coordinate frame must be sent to the client as well.  With these two pieces of information the visualization of the vehicle in the three dimensional environment is possible.

\subsection{Rviz}
Rviz\cite{rviz} is a visual debugging tool provided by the Robotic Operating System (ROS)\cite{quigley2009ros}.  ROS is used at large in the experimental implementation of this system.  Rviz provides interfaces to visualize generic data types commonly found in robotics, like point clouds, laser scans, odometry, transforms, and others.  In addition to these generic data formats, rviz allows for more general drawing programmatically using primitive shapes like cylinders, spheres, boxes, and points.  More advanced shapes and custom shapes are also allowed.  These shapes can be seen in Figure \ref{fig:rviz_shapes}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=6in,keepaspectratio]{rviz_shapes.png}
  \caption{Demo of drawing types in rviz\cite{rviz_shapes}.}
  \label{fig:rviz_shapes}
\end{figure}

Since the underlying system is already using ROS and rviz provides methods for drawing custom three dimensional information, rviz was chosen as the basis for the visualization system.

\subsection{Rendering the Map}
In order to visualize or render the map using rviz, the map data must be put into a format that rviz supports.  There is no standard data type that satisfies the display of an octree.  Though each leaf of the octree could be represented as a point in a point cloud, each leaf also can have different sizes - i.e. not all leaves in the octree are the lowest depth of the tree.  Therefore custom drawing tools are required for drawing the octree correctly.

For efficiency in drawing, one of the custom drawing types is a `cube list', in contrast to just a `cube' drawing type.  In a cube list, a single message is sent to rviz which is of type `cube list' and has an array of positions and colors for each cube in the list.  Sending cube lists is far more efficient than sending a message to rviz for each leaf in the octree.  Each cube in a cube list, however, must be the same size.  Therefore, a cube list must exist for each level of the octree.

\begin{algorithm}
\caption{Algorithm for Drawing an Octree in Rviz}
\label{alg:octree_rviz}
\begin{algorithmic}
  \STATE \COMMENT {$O$ is the client map octree}
  \STATE \COMMENT {$cubelists$ is an array of leaf sets})
  \FOR{$leaf$ in $leaves(O)$}
    \STATE $cubelists\{size(leaf)\} = cubelists\{size(leaf)\} \cap leaf$
  \ENDFOR
  \FOR{$i=0$ to $height(O)$}
    \STATE $sendCubeList(cubelists\{i\}, size(cubelists\{i\}\{0\}))$
  \ENDFOR
\end{algorithmic}
\end{algorithm}

The algorithm for drawing an octree in rviz is displayed in Algorithm \ref{alg:octree_rviz}.  In the algorithm the $leaves$ function returns an array of the leaves in the given octree, the $size$ function returns the dimensions of the voxel represented by a given leaf, and the $sendCubeList$ function sends the given set of leaves to rviz to be rendered at the given size.  This algorithm sorts the leaves of the octree into sets by height.  Then each set of leaves can be sent as one `cube list' of the appropriate size for that height to be rendered in rviz.

\subsection{Coloring the Octree}
When rendering the octree, a few options are available on how to color them.  One obvious option is to make all the leaves the same flat color.  a single color makes it difficult to see the three dimensional structure, so this color method is not used.  Another option is to color the voxels of the octree by height in the global coordinate frame.  The height map coloring gives some context to the voxels, where the lowest to the ground voxels will be one color and voxels near the ceiling will be a different color.  The height map coloring is the most effective method of rendering the voxels such that the teleoperator can understand the three dimensional structure.

Another option is to color the voxels by the color they would have in the real environment.  Per point coloring requires that the system, like the Kinect or Xtion Pro Live, provides color per point in the incoming point clouds.  This information is stored in the octree as an average of the colors of points that raytraced to each leaf.  This representation can add a more photo realistic quality to the environment for the teleoperator but consumes additional bandwidth and does not render well when the map is at a low resolution.

\section{Teleoperator Input}
The final responsibility of the teleoperation subsystem is to collect the input from the teleoperator and transmit the input to the robotic vehicle.  Capturing the input is achieved using a Human Interface Device (HID) like a gamepad or a joystick.  In addition to a gamepad or joystick, a more vehicle like system could be used like a steering wheel and pedals.  The input from one of these devices is mapped to linear and angular velocities that are appropriate for the vehicle being teleoperated.  These linear and angular velocities are sent to the robotic vehicle when they are executed by that subsystem.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Chapter: Latency Reduction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Latency Reduction}\label{chap:latency_reduction}
This chapter talks about latency reduction techniques and their implementation.

\section{Vehicle Model}
Talk about the kinematic vehicle model.

\section{Interface}
How is the model used to help the teleoperator?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Chapter: Experiments
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Experiments}\label{chap:experiments}
NOTE: This chapter needs to be completely rewritten, disregard.

In this chapter the experimental goals, setup, and results are presented.  Each experiment is presented with the goals, setup, and results all together.  There are several experiments presented here, some are tests of the subsystems, while others are experiments to test aspects of the system as a whole.  The order of these experiments tend to follow the order of implementation.

\section{Robotic Vehicle Tests}
\label{sec:robotic_vehicle}
One of the first components needed for further experiments is the robotic vehicle.  The goals of these tests are to show that the robotic vehicle platform is ready for further experiments regarding the mapping and teleoperation subsystems.

\subsection{SegwayRMP 200}
There have been three robotic vehicle platforms during the course of this research.  The first system was based on a Segway RMP200 ATV platform.  This system had a Mac Mini with a 2.6GHz Core 2 Duo processor.  The sensor package consisted of a Sick LMS151, wheel odometry, and a Microsoft Kinect.  This platform was electrically damaged a few months into the research and new platform had to be pursued.  No data collected with this system was used in the final research, but it served as a learning platform when the first proof of concepts for this work were underway.  Additionally, a software artifact in the form of the library `libsegwayrmp' resulted from this part of the work.  The library is middleware agnostic, i.e. not ROS or MOOS specific, and is in use at as many as ten different institutes outside of Auburn to date.

\begin{figure}[ht]
  \centering
  \includegraphics[width=4in,keepaspectratio]{segway.jpg}
  \caption{Segway RMP200 ATV Robotic Vehicle Platform}
  \label{fig:segway_rmp200}
\end{figure}

\subsection{Autonomous Lawnmower}
The next platform target was to be an iRobot ATRV which needed a lot of work before it would be useful for research.  In the mean time to collect data for a publication, the Auburn autonomous lawnmower was used as a robotic vehicle platform for a short time.  Data taken with the lawnmower is used in the final research and was used in the ION PLANS publication.  Like the Segway platform, the lawnmower had the Sick LMS151, the Microsoft Kinect, and wheel odometry.  Additionally, it had the Point Grey Research Bumblebee2 stereo vision system.  The lawnmower had significantly poorer odometry, due to low resolution encoders, this increased the error discussed in the Section \ref{sec:uncertainty}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=4in,keepaspectratio]{lawnmower.png}
  \caption{Autonomous Lawnmower Mower Robotic Vehicle Platform}
  \label{fig:lawnmower}
\end{figure}

\subsection{iRobot ATRV}
The final implementation of the robotic vehicle platform was based on the iRobot ATRV.  This platform is the one used in the final experiments, and is intended to the the host of the system for future work.  The ATRV is equipped with the Sick LMS151, high resolution wheel encoders, the Microsoft Kinect and the ASUS Xtion Pro Live.  The higher resolution encoders provided a much better pose estimate between SLAM updates, resulting in much better results during mapping.

\begin{figure}[ht]
  \centering
  \includegraphics[width=4in,keepaspectratio]{atrv.jpeg}
  \caption{ATRV Robotic Vehicle Platform}
  \label{fig:atrv}
\end{figure}

\subsection{Software}
All of the robotic platforms used very similar software.  The middleware used on all of the robotic platforms was ROS, the Robotics Operating System.  ROS provides a build system, code organization tools, inter process communication, standard data types common in robotics, debugging tools, visualization tools, implementations of common algorithms, and a large open community.  Because of the common ground provided by ROS and its tools, many software packages were available to speed up the development of the robotic platforms.  For example, there exists a driver for the Sick LMS series laser range finders, so there was no work there, and more importantly there is excellent support for RGBD sensors like the Kinect and the Xtion Pro Live.  With drivers for the Kinect and tight integration with perception libraries like octomap and PCL, ROS was the best engineering decision for the robotic platform and this work.

Setting up the vehicle in ROS involved finding or writing drivers for all of the hardware, describing the robot's geometry in the Unified Robot Description Format (URDF), and drawing a model of the robot in blender for use in the teleoperation visualization, see Figure \ref{fig:atrv_blender}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=4in,keepaspectratio]{atrv_blender.png}
  \caption{ATRV Robotic Vehicle Platform in Blender}
  \label{fig:atrv_blender}
\end{figure}

Once the robot was setup with a URDF and all of the hardware interfaces were working, the vehicle was ready to have its sensors visualized and recorded.  Using rviz the sensors on the vehicle can be visualize because they all adhere to the common data types laid out in ROS.  Additionally, the logging system provided by ROS was able to keep up with the Kinect and laser data, which was a concern.  The Kinect, and similar sensors, produce significant amounts of data.  Table \ref{tab:kinect_data} shows how much data is produces by the Kinect.  Recording the whole system results in log files that are 2 to 4 gigabytes per minute in size depending on the resolution.

\begin{table}
\caption{Bandwidth of Raw Kinect Data at Different Resolutions}
\label{tab:kinect_data}
\begin{center}
  \begin{tabular}{ | l | l | c | }
    \hline
    ~ & Resolution & Megabytes per second \\
    \hline
    VGA & 640x480 & 76.6 \\
    QVGA & 320x240 & 50.5 \\
    QQVGA & 160x120 & 30.5 \\
    \hline
  \end{tabular}
\end{center}
\end{table}

After the system is setup for teleoperating and recording, data can be collected to test the next subsystem, the mapping subsystem, off-line.

\section{Mapping}
Much of the testing of the mapping system is done with recorded data being played back.  This allows for iterating over specific sections of data sets to try and test specific behaviors of the mapping system, and to try and measure the effects of different settings.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Chapter: Conclusions
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Conclusion}\label{chap:conclusion}
This chapter draws some conclusions and looks to future work.

\section{Future Work}
Some thing about future work.

\subsection{Improved Mapping}
Talk about KinectFusion again.

\subsection{Dynamic Vehicle Models}
Talk about how a dynamic vehicle model might improve the latency reduction system.

\subsection{More Latency Reduction Studies}
More data is required to properly quantify the latency reduction system's effectiveness.  (Likely)

%=================
%=================
%======fin========
%=================
%=================

%If you do not need the List of Abbreviations\nomenclature{LoA}{List of Abbreviations}, comment the nomencl package and associated nomenclature commands. 


%%%%%%%%Two options for having a bibliography. If you use a separate file or multiple files:

%%%%% where the files are robotics.bib, imageprocessing.bib and/or thesis.bib. 
\bibliographystyle{IEEEtran}
\bibliography{references}
%Or you can include the bibliography entries directly:

\appendix
\chapter*{Appendices\addcontentsline{toc}{chapter}{Appendices}}
\chapter{Some C++ Source Code}
\begin{singlespace}
\begin{verbatim}
#include <iostream>

using std::cout;
using std::endl;

int main(void) {
  cout << "Hello World." << endl;
  return 0;
}
\end{verbatim}
\end{singlespace}

\end{document}

